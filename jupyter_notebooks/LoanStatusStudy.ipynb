{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Loan Status Study Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## General Idea\n",
        "\n",
        "* A company aims to revolutionize its loan qualification process by automating and streamlining the assessment of online loan applications in real-time. The focus is on employing machine learning (ML) models that can accurately predict whether a loan should be approved for an applicant, based on the information provided during the application process. This initiative seeks to expedite the decision-making process, ensuring quick and efficient determination of loan eligibility.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "* Business Requirement:\n",
        "Conduct exploratory data analysis (EDA) to understand patterns and trends in the data, such as the impact of education level, marital status, and income on loan approval. Therefore analyze the given data to identify common characteristics of applicants who default versus those who repay successfully. Try to make conclusions.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/LoanStatusPrediction.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate code/ plots to underline they key patterns of the dataset. The output should give some evidence concerning the business requirement.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "The Load_ID variable is unique to each dataset and therefore will be dropped for the further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/LoanStatusPrediction.csv\")\n",
        "    .drop(['Loan_ID'], axis=1)\n",
        "    )\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration\n",
        "\n",
        "We are interested to get more familiar with the dataset, check variable type and distribution, missing levels and what these variables mean in a business context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=df, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handling of missing Data\n",
        "\n",
        "Handling missing data is crucial for the upcoming Correlation Study to work, several strategies exist.\n",
        "* Dropping missing values\n",
        "* Imputation, e.g. with the help of mean/ median\n",
        "* Predictive Imputation\n",
        "\n",
        "In the following a simple imputation technique is used, based on `median` for numerical variables and `most_frequent` for categorical variables. The imputation takes place for missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from feature_engine.encoding import OneHotEncoder\n",
        "\n",
        "# For numerical variables\n",
        "imputer_num = SimpleImputer(strategy='median')\n",
        "numerical_vars = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numerical_vars] = imputer_num.fit_transform(df[numerical_vars])\n",
        "\n",
        "# For categorical variables\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "categorical_vars = df.select_dtypes(include=['object']).columns\n",
        "df[categorical_vars] = imputer_cat.fit_transform(df[categorical_vars])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Correlation Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.encoding import OneHotEncoder\n",
        "encoder = OneHotEncoder(variables=df.columns[df.dtypes=='object'].to_list(), drop_last=False)\n",
        "df_ohe = encoder.fit_transform(df)\n",
        "print(df_ohe.shape)\n",
        "df_ohe.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make use of `.corr()` for `spearman` and `pearson` methods and investigate the top 10 correlations\n",
        "* This command returns a pandas series and the first item is the correlation between target and target, therfore target (Loan_Status) gets excluded. \n",
        "* We sort values considering the absolute value, by setting `key=abs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_spearman = df_ohe.corr(method='spearman')['Loan_Status'].sort_values(key=abs, ascending=False)[1:].head(10)\n",
        "corr_spearman"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same goes for `pearson`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_pearson = df_ohe.corr(method='pearson')['Loan_Status'].sort_values(key=abs, ascending=False)[1:].head(10)\n",
        "corr_pearson"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both methods show up weak or moderate levels of correlation between Loan_Status and a given variable. \n",
        "* With `Credit_History` being the only given feature showing highest correlation, which can be alos argumented causally.\n",
        "\n",
        "The top five correlation levels will be pursued in `df_ohe` and the associated variables will be studied at `df`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_n = 5\n",
        "set(corr_pearson[:top_n].index.to_list() + corr_spearman[:top_n].index.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Addressing the Top 5 Selection\n",
        "The goal was to select the top five variables, but due to overlaps six were considered.\n",
        "Therefore a more logical approach is pushed forward:\n",
        "1. Since Credit_History appears as the most correlated variable in both analyses, it should definitely be included.\n",
        "2. Acknowledging that some variables like Married_Yes/Married_No are essentially the same feature, they should be counted as one, `Married`.\n",
        "3. Property_Area should be considered as one variable, though it has been split into Semiurban and Rural due to one-hot encoding. Including both as separate entities might be redundant for a top 5 analysis where you're looking for diversity in features.\n",
        "4. Since CoapplicantIncome shows a significant correlation in the Spearman analysis and represents a different aspect of the applicants financial situation, it's reasonable to include it for further study despite it increasing the count beyond five when combining lists.\n",
        "\n",
        "### Therefore following four variables will be further studied and investigated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_to_study = ['Credit_History', 'CoapplicantIncome', 'Married', 'Property_Area']\n",
        "vars_to_study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA on selected variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_eda = df.filter(vars_to_study + ['Loan_Status'])\n",
        "df_eda.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variables Distribution by Churn\n",
        "\n",
        "The distribution is plotted (numerical and categorical) coloured by Loan_Status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "\n",
        "def plot_categorical(df, col, target_var):\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    sns.countplot(data=df, x=col, hue=target_var, order=df[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.title(f\"{col}\", fontsize=20, y=1.05)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_numerical(df, col, target_var):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.histplot(data=df, x=col, hue=target_var, kde=True, element=\"step\")\n",
        "    plt.title(f\"{col}\", fontsize=20, y=1.05)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "target_var = 'Loan_Status'\n",
        "for col in vars_to_study:\n",
        "    if df_eda[col].dtype == 'object':\n",
        "        plot_categorical(df_eda, col, target_var)\n",
        "        print(\"\\n\\n\")\n",
        "    else:\n",
        "        plot_numerical(df_eda, col, target_var)\n",
        "        print(\"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions\n",
        "\n",
        "The analysis of correlations and visual data reveals consistent findings:\n",
        "\n",
        "* Having a good credit history appears to positively affect loan approval.\n",
        "* The absence of income for the co-applicant could negatively impact the chances of securing a loan.\n",
        "* Being married is seen to positively influence loan approval prospects.\n",
        "* Residents of suburban areas are more likely to be approved for loans."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
